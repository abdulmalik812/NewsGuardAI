{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting an overview of how the data looks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hous dem aid even see comey letter jason chaff...</td>\n",
       "      <td>hous dem aid even see comey letter jason chaff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flynn hillari clinton big woman campus breitbart</td>\n",
       "      <td>ever get feel life circl roundabout rather hea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>truth might get fire</td>\n",
       "      <td>truth might get fire octob 29 2016 tension int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15 civilian kill singl us airstrik identifi</td>\n",
       "      <td>video 15 civilian kill singl us airstrik ident...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iranian woman jail fiction unpublish stori wom...</td>\n",
       "      <td>print iranian woman sentenc six year prison ir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  hous dem aid even see comey letter jason chaff...   \n",
       "1   flynn hillari clinton big woman campus breitbart   \n",
       "2                               truth might get fire   \n",
       "3        15 civilian kill singl us airstrik identifi   \n",
       "4  iranian woman jail fiction unpublish stori wom...   \n",
       "\n",
       "                                                body  label  \n",
       "0  hous dem aid even see comey letter jason chaff...      1  \n",
       "1  ever get feel life circl roundabout rather hea...      0  \n",
       "2  truth might get fire octob 29 2016 tension int...      1  \n",
       "3  video 15 civilian kill singl us airstrik ident...      1  \n",
       "4  print iranian woman sentenc six year prison ir...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stemmed, lemmatized datasets were created in the previous notebook\n",
    "\n",
    "df_stemmed = pd.read_csv('D:/MajorProject_Code/Data/stemmed_clean_data.csv') \n",
    "df_lemmatized = pd.read_csv('D:/MajorProject_Code/Data/lemmatized_clean_data.csv')\n",
    "df_stemmed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Checking for and dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      5\n",
       "body     710\n",
       "label      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking null values for stemmed\n",
    "\n",
    "df_stemmed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    0\n",
       "body     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stemmed = df_stemmed.dropna()\n",
    "df_stemmed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      5\n",
       "body     710\n",
       "label      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking null values for lemmatized\n",
    "\n",
    "df_lemmatized.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    0\n",
       "body     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lemmatized = df_lemmatized.dropna()\n",
    "df_lemmatized.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed cleaned data shape: (68430, 3)\n",
      "Lemmatized cleaned data shape: (68430, 3)\n"
     ]
    }
   ],
   "source": [
    "#Shape of stemmed and Lematized\n",
    "\n",
    "print('Stemmed cleaned data shape:', df_stemmed.shape)\n",
    "print('Lemmatized cleaned data shape:', df_lemmatized.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As we can see, we've dropped all the missing values*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We split the data into train and test(80:20 respectively) to perform furthur feature extraction and checking out the shapes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of X_train (54744, 2) \n",
      " Shape of y_train (54744,) \n",
      " Shape of X_test (13686, 2) \n",
      " Shape of y_test (13686,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_stemmed, X_lemmatized = df_stemmed.drop('label', axis = 1), df_lemmatized.drop('label', axis = 1)\n",
    "y_stemmed, y_lemmatized = df_stemmed['label'], df_lemmatized['label']\n",
    "\n",
    "#Train, test split for stemmed\n",
    "X_stemmed_train, X_stemmed_test, y_stemmed_train, y_stemmed_test = train_test_split(X_stemmed, y_stemmed, test_size = 0.2, random_state = 42, stratify = y_stemmed)\n",
    "\n",
    "#Train, test split for lemmatized\n",
    "X_lemmatized_train, X_lemmatized_test, y_lemmatized_train, y_lemmatized_test = train_test_split(X_lemmatized, y_lemmatized, test_size = 0.2, random_state = 42, stratify = y_lemmatized)\n",
    "\n",
    "print(\" Shape of X_train {} \\n Shape of y_train {} \\n Shape of X_test {} \\n Shape of y_test {}\" .format(X_lemmatized_train.shape, y_lemmatized_train.shape, X_lemmatized_test.shape, y_lemmatized_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we work on the train data to extract features that can be used while modeling to get better results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some featurization techniques that we're going to be trying on the title and body**\n",
    "\n",
    "1. **Tf-idf** -  Evaluates how relevant a word is to a document in a collection of documents.\n",
    "2. **Word2Vec** - uses a neural network model to learn word associations from a large corpus of text. Takes into consideration the semantic meaning\n",
    "3. **Tf-idf weighted word2Vec** - Word2Vec gives us the representation of each word in vector format, but since we have sequential information, we need to convert sentences to vectors which can be done using this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tf-idf feature extraction for the title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tf-idf** feature extraction from the title including unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after getting Tfidf values with n-grams tokenization (54744, 13735)\n",
      "Some sample features(unique words in the corpus) ['000', '000 email', '000 illeg', '000 job', '000 migrant', '000 peopl', '000 refuge', '000 roberto', '000 rohingya', '000 year']\n",
      "The number of unique words including both unigrams and bigrams  13735\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "title_tfidf = TfidfVectorizer(ngram_range=(1,2), min_df = 0.0001)\n",
    "final_title_tfidf = title_tfidf.fit_transform(X_stemmed_train['title'].values.astype('U'), )  \n",
    "print('Shape after getting Tfidf values with n-grams tokenization', final_title_tfidf.get_shape())\n",
    "print(\"Some sample features(unique words in the corpus)\",title_tfidf.get_feature_names()[0:10])\n",
    "print(\"The number of unique words including both unigrams and bigrams \", final_title_tfidf.get_shape()[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(title_tfidf, open(\"title_tfidf.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tf-idf** feature extraction from the title not including unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after getting Tfidf values with n-grams tokenization (13686, 13735)\n",
      "Some sample features(unique words in the corpus) ['000', '000 email', '000 illeg', '000 job', '000 migrant', '000 peopl', '000 refuge', '000 roberto', '000 rohingya', '000 year']\n",
      "The number of unique words including both unigrams and bigrams  13735\n"
     ]
    }
   ],
   "source": [
    "#vectorizing test data\n",
    "\n",
    "title_test_tfidf = title_tfidf.transform(X_stemmed_test['title'].values.astype('U'), )\n",
    "print('Shape after getting Tfidf values with n-grams tokenization', title_test_tfidf.get_shape())\n",
    "print(\"Some sample features(unique words in the corpus)\",title_tfidf.get_feature_names()[0:10])\n",
    "print(\"The number of unique words including both unigrams and bigrams \", title_test_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after getting Tfidf values with n-grams tokenization (54744, 106)\n",
      "Some sample features(unique words in the corpus) ['america', 'american', 'anti', 'arrest', 'ask', 'attack', 'back', 'ban', 'bill', 'black']\n",
      "The number of unique words without bigrams  106\n"
     ]
    }
   ],
   "source": [
    "title_tfidf2 = TfidfVectorizer(min_df = 0.01)\n",
    "final_title_tfidf2 = title_tfidf2.fit_transform(X_stemmed_train['title'].values.astype('U'))\n",
    "print('Shape after getting Tfidf values with n-grams tokenization', final_title_tfidf2.get_shape())\n",
    "print(\"Some sample features(unique words in the corpus)\",title_tfidf2.get_feature_names()[0:10])\n",
    "print(\"The number of unique words without bigrams \", final_title_tfidf2.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after getting Tfidf values with n-grams tokenization (13686, 106)\n",
      "Some sample features(unique words in the corpus) ['america', 'american', 'anti', 'arrest', 'ask', 'attack', 'back', 'ban', 'bill', 'black']\n",
      "The number of unique words including both unigrams and bigrams  106\n"
     ]
    }
   ],
   "source": [
    "#vectorizing test data\n",
    "\n",
    "title_test_tfidf2 = title_tfidf2.transform(X_stemmed_test['title'].values.astype('U'), )  \n",
    "print('Shape after getting Tfidf values with n-grams tokenization', title_test_tfidf2.get_shape())\n",
    "print(\"Some sample features(unique words in the corpus)\",title_tfidf2.get_feature_names()[0:10])\n",
    "print(\"The number of unique words including both unigrams and bigrams \", title_test_tfidf2.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Observations and Conclusions:**\n",
    "1. The number of unique words from the corpus with unigrams and bigrams is almost 14times bigger than the ones without unigrams and bigrams.\n",
    "2. The increased features might help us get better results with the cost in computation\n",
    "3. Also, we have to keep inmind that this is just the title featurized and the body will create even more\n",
    "4. So, because of this exploding features problem, we'll have to tune the min_df parameter so that we can eleminate words which occur in less number of documents thereby decreasing some features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tf-idf feature extraction for the body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after getting Tfidf values with n-grams tokenization (54744, 32806)\n",
      "Some sample features(unique words in the corpus) ['00', '00 00', '00 eastern', '00 pm', '000', '000 000', '000 acr', '000 american', '000 barrel', '000 campaign']\n",
      "The number of unique words including both unigrams and bigrams  32806\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "body_tfidf = TfidfVectorizer(ngram_range=(1,2), min_df = 0.001)\n",
    "final_body_tfidf = body_tfidf.fit_transform(X_stemmed_train['body'].values.astype('U'))   \n",
    "print('Shape after getting Tfidf values with n-grams tokenization', final_body_tfidf.get_shape())\n",
    "print(\"Some sample features(unique words in the corpus)\",body_tfidf.get_feature_names()[0:10])\n",
    "print(\"The number of unique words including both unigrams and bigrams \", final_body_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(body_tfidf, open(\"body_tfidf.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after getting Tfidf values with n-grams tokenization (13686, 32806)\n",
      "Some sample features(unique words in the corpus) ['00', '00 00', '00 eastern', '00 pm', '000', '000 000', '000 acr', '000 american', '000 barrel', '000 campaign']\n",
      "The number of unique words including both unigrams and bigrams  32806\n"
     ]
    }
   ],
   "source": [
    "#vectorizing test data\n",
    "\n",
    "body_test_tfidf = body_tfidf.transform(X_stemmed_test['body'].values.astype('U'), )  \n",
    "print('Shape after getting Tfidf values with n-grams tokenization', body_test_tfidf.get_shape())\n",
    "print(\"Some sample features(unique words in the corpus)\",body_tfidf.get_feature_names()[0:10])\n",
    "print(\"The number of unique words including both unigrams and bigrams \", body_test_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after getting Tfidf values with n-grams tokenization (54744, 3053)\n",
      "Some sample features(unique words in the corpus) ['000', '10', '100', '11', '12', '13', '14', '15', '150', '16']\n",
      "The number of unique words without bigrams  3053\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "body_tfidf2 = TfidfVectorizer(min_df = 0.01)\n",
    "final_body_tfidf2 = body_tfidf2.fit_transform(X_stemmed_train['body'].values.astype('U'))\n",
    "print('Shape after getting Tfidf values with n-grams tokenization', final_body_tfidf2.get_shape())\n",
    "print(\"Some sample features(unique words in the corpus)\",body_tfidf2.get_feature_names()[0:10])\n",
    "print(\"The number of unique words without bigrams \", final_body_tfidf2.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after getting Tfidf values with n-grams tokenization (13686, 3053)\n",
      "Some sample features(unique words in the corpus) ['000', '10', '100', '11', '12', '13', '14', '15', '150', '16']\n",
      "The number of unique words including both unigrams and bigrams  3053\n"
     ]
    }
   ],
   "source": [
    "#vectorizing test data\n",
    "\n",
    "body_test_tfidf2 = body_tfidf2.transform(X_stemmed_test['body'].values.astype('U'), )  \n",
    "print('Shape after getting Tfidf values with n-grams tokenization', body_test_tfidf2.get_shape())\n",
    "print(\"Some sample features(unique words in the corpus)\",body_tfidf2.get_feature_names()[0:10])\n",
    "print(\"The number of unique words including both unigrams and bigrams \", body_test_tfidf2.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After a bit of tuning the 'min_df' parameter, we come to a conclusion to stick with the above values of min_df and we select the 'final_title_tfidf' and 'final_body_tfidf' and decide to go with these features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Combining and saving the selected features so that it can be used to feed the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the combined data after combining the tfidf body and title features (54744, 46541)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "tfidf_features = hstack((final_title_tfidf, final_body_tfidf))\n",
    "print('The shape of the combined data after combining the tfidf body and title features', tfidf_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the combined data after combining the tfidf body and title features (54744, 3159)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "tfidf_features2 = hstack((final_title_tfidf2, final_body_tfidf2))\n",
    "print('The shape of the combined data after combining the tfidf body and title features', tfidf_features2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.save_npz.html#scipy.sparse.save_npz\n",
    "import scipy.sparse\n",
    "\n",
    "scipy.sparse.save_npz('D:/MajorProject_Code/Data/tfidf_features.npz', tfidf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the combined data after combining the tfidf body and title features (13686, 46541)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "test_tfidf_features = hstack((title_test_tfidf, body_test_tfidf))\n",
    "print('The shape of the combined data after combining the tfidf body and title features', test_tfidf_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.save_npz.html#scipy.sparse.save_npz\n",
    "import scipy.sparse\n",
    "\n",
    "scipy.sparse.save_npz('D:/MajorProject_Code/Data/tfidf_features2.npz', tfidf_features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.save_npz.html#scipy.sparse.save_npz\n",
    "import scipy.sparse\n",
    "\n",
    "scipy.sparse.save_npz('D:/MajorProject_Code/Data/test_tfidf_features.npz', test_tfidf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the combined data after combining the tfidf body and title features (13686, 3159)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "test_tfidf_features2 = hstack((title_test_tfidf2, body_test_tfidf2))\n",
    "print('The shape of the combined data after combining the tfidf body and title features', test_tfidf_features2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.save_npz.html#scipy.sparse.save_npz\n",
    "import scipy.sparse\n",
    "\n",
    "scipy.sparse.save_npz('D:/MajorProject_Code/Data/test_tfidf_features2.npz', test_tfidf_features2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Word2Vec on Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#Preparing the data\n",
    "\n",
    "lemmatized_title = []\n",
    "for sentence in X_lemmatized_train['title']:\n",
    "    lemmatized_title.append(sentence.split())\n",
    "\n",
    "print(type(lemmatized_title[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#Preparing the test data\n",
    "\n",
    "lemmatized_title_test = []\n",
    "for sentence in X_lemmatized_test['title']:\n",
    "    lemmatized_title_test.append(sentence.split())\n",
    "\n",
    "print(type(lemmatized_title_test[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can see below that we have got out data in the form that is required for word2Vec training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['military', 'expert', 'vow', 'take', 'trump', 'challenge']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_title[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'military expert vow take trump challenge'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lemmatized_train['title'][49668]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Training the Word2Vec model on our corpus to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "title_word2vec = Word2Vec(lemmatized_title, min_count = 5, size = 100, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8847\n"
     ]
    }
   ],
   "source": [
    "words = list(title_word2vec.wv.vocab)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words that occured minimum 5 times  8847\n",
      "sample words  ['military', 'expert', 'vow', 'take', 'trump', 'challenge', 'duterte', 'promise', 'china', 'not']\n"
     ]
    }
   ],
   "source": [
    "w2v_titlewords = list(title_word2vec.wv.vocab)\n",
    "print(\"number of words that occured minimum 5 times \",len(w2v_titlewords))\n",
    "print(\"sample words \", w2v_titlewords[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('world', 0.8842624425888062),\n",
       " ('isi', 0.8627840280532837),\n",
       " ('syria', 0.8149207830429077),\n",
       " ('aleppo', 0.8086670637130737),\n",
       " ('iii', 0.8062310814857483),\n",
       " ('airstrikes', 0.7967962622642517),\n",
       " ('weapon', 0.7873474359512329),\n",
       " ('elsewhere', 0.7864370346069336),\n",
       " ('chemical', 0.7854568362236023),\n",
       " ('civilian', 0.7726395726203918)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at how the model performs - testing it with some common words\n",
    "\n",
    "title_word2vec.wv.most_similar('war')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('obama', 0.7696099281311035),\n",
       " ('jr', 0.6320515871047974),\n",
       " ('penny', 0.6298249363899231),\n",
       " ('vice', 0.6091779470443726),\n",
       " ('inherit', 0.6086440086364746),\n",
       " ('elect', 0.585789680480957),\n",
       " ('ivana', 0.5709762573242188),\n",
       " ('president', 0.5678825378417969),\n",
       " ('roar', 0.5675894021987915),\n",
       " ('ted', 0.5599625110626221)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_word2vec.wv.most_similar('trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_word2vec.save(\"D:/MajorProject_Code/Data//title_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Observations/ Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We can see from the above results that our word2Vec trained on title is doing a fairly good job and giving out similar/related words\n",
    "2. The same model when trained on the body is expected to do much better since the body has a huge text corpus compared to the title\n",
    "3. We can decrease the min_count in order to get some more words in but we decide to keep it here as we want to avoid blasting features as we will be combining the body word2vec features too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Word2Vec on Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#Preparing the data for body\n",
    "\n",
    "lemmatized_body = []\n",
    "for sentence in X_lemmatized_train['body']:\n",
    "    lemmatized_body.append(sentence.split())\n",
    "\n",
    "print(type(lemmatized_body[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#Preparing the test data for body\n",
    "\n",
    "lemmatized_body_test = []\n",
    "for sentence in X_lemmatized_test['body']:\n",
    "    lemmatized_body_test.append(sentence.split())\n",
    "\n",
    "print(type(lemmatized_body_test[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "body_word2vec = Word2Vec(lemmatized_body, min_count = 10, size = 100, workers = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40427\n"
     ]
    }
   ],
   "source": [
    "bodywords = list(body_word2vec.wv.vocab)\n",
    "print(len(bodywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words that occured minimum 5 times  40427\n",
      "sample words  ['donald', 'trump', 'like', 'bully', 'love', 'talk', 'crap', 'victim', 'stand', 'always']\n"
     ]
    }
   ],
   "source": [
    "w2v_bodywords = list(body_word2vec.wv.vocab)\n",
    "print(\"number of words that occured minimum 5 times \",len(w2v_bodywords))\n",
    "print(\"sample words \", w2v_bodywords[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('invasion', 0.6442214250564575),\n",
       " ('conflict', 0.5576161742210388),\n",
       " ('warfare', 0.5554473400115967),\n",
       " ('unrest', 0.5446168184280396),\n",
       " ('disobedience', 0.5333037376403809),\n",
       " ('ww', 0.5145142674446106),\n",
       " ('invade', 0.5143904089927673),\n",
       " ('reign', 0.5110424757003784),\n",
       " ('wwii', 0.5090429782867432),\n",
       " ('blooded', 0.5065775513648987)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at how the model performs - testing it with some common words\n",
    "\n",
    "body_word2vec.wv.most_similar('war')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elect', 0.6706319451332092),\n",
       " ('obama', 0.568300724029541),\n",
       " ('clinton', 0.54697585105896),\n",
       " ('penny', 0.5380555391311646),\n",
       " ('bush', 0.517288327217102),\n",
       " ('conway', 0.512274980545044),\n",
       " ('republican', 0.5119106769561768),\n",
       " ('cruz', 0.5015451312065125),\n",
       " ('sander', 0.4889506697654724),\n",
       " ('rubio', 0.4801648259162903)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_word2vec.wv.most_similar('trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.Word2Vec"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(body_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_word2vec.save(\"D:/MajorProject_Code/Data//body_word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Computing Average-word2Vec and Tfidf-weighted-word2Vec from the values obtained above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We have got the vector representations of individual words from the word2Vec model, since we have sequential data, we need to compute the vectors of the sequences*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54744\n",
      "100\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# average Word2Vec for title\n",
    "\n",
    "title_avgvectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in lemmatized_title:# for each review/sentence\n",
    "    sent_vec = np.zeros(100) \n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_titlewords:\n",
    "            vec = title_word2vec.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    title_avgvectors.append(sent_vec)\n",
    "print(len(title_avgvectors))\n",
    "print(len(title_avgvectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13686\n",
      "100\n",
      "Wall time: 3.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# average Word2Vec for test title\n",
    "\n",
    "test_title_avgvectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent2 in lemmatized_title_test:# for each review/sentence\n",
    "    sent_vec2 = np.zeros(100) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n",
    "    cnt_words2 =0; # num of words with a valid vector in the sentence/review\n",
    "    for word2 in sent2: # for each word in a review/sentence\n",
    "        if word2 in w2v_titlewords:\n",
    "            vec2 = title_word2vec.wv[word2]\n",
    "            sent_vec2 += vec2\n",
    "            cnt_words2 += 1\n",
    "    if cnt_words2 != 0:\n",
    "        sent_vec2 /= cnt_words2\n",
    "    test_title_avgvectors.append(sent_vec2)\n",
    "print(len(test_title_avgvectors))\n",
    "print(len(test_title_avgvectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54744\n",
      "100\n",
      "Wall time: 19min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# average Word2Vec for body\n",
    "\n",
    "body_avgvectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent2 in lemmatized_body:# for each review/sentence\n",
    "    sent_vec2 = np.zeros(100) \n",
    "    cnt_words2 =0; # num of words with a valid vector in the sentence/review\n",
    "    for word2 in sent2: # for each word in a review/sentence\n",
    "        if word2 in w2v_bodywords:\n",
    "            vec2 = body_word2vec.wv[word]\n",
    "            sent_vec2 += vec2\n",
    "            cnt_words2 += 1\n",
    "    if cnt_words2 != 0:\n",
    "        sent_vec2 /= cnt_words2\n",
    "    body_avgvectors.append(sent_vec2)\n",
    "print(len(body_avgvectors))\n",
    "print(len(body_avgvectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13686\n",
      "100\n",
      "Wall time: 4min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# average Word2Vec for test body\n",
    "\n",
    "test_body_avgvectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent3 in lemmatized_body_test:# for each review/sentence\n",
    "    sent_vec3 = np.zeros(100) \n",
    "    cnt_words3 =0; # num of words with a valid vector in the sentence/review\n",
    "    for word3 in sent3: # for each word in a review/sentence\n",
    "        if word3 in w2v_bodywords:\n",
    "            vec3 = body_word2vec.wv[word3]\n",
    "            sent_vec3 += vec3\n",
    "            cnt_words3 += 1\n",
    "    if cnt_words3 != 0:\n",
    "        sent_vec3 /= cnt_words3\n",
    "    test_body_avgvectors.append(sent_vec3)\n",
    "print(len(test_body_avgvectors))\n",
    "print(len(test_body_avgvectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting title and body to sparse matrices and combining them\n",
    "import scipy\n",
    "\n",
    "title_w2v = scipy.sparse.csr_matrix(title_avgvectors)\n",
    "body_w2v = scipy.sparse.csr_matrix(body_avgvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the combined data after combining the tfidf body and title features (54744, 200)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "avgw2v_features = hstack((title_w2v, body_w2v))\n",
    "print('The shape of the combined data after combining the tfidf body and title features', avgw2v_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the Extracted features\n",
    "\n",
    "scipy.sparse.save_npz('D:/MajorProject_Code/Data/avg_w2v.npz', avgw2v_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting test title and body to sparse matrices and combining them\n",
    "import scipy\n",
    "\n",
    "test_title_w2v = scipy.sparse.csr_matrix(test_title_avgvectors)\n",
    "test_body_w2v = scipy.sparse.csr_matrix(test_body_avgvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the combined data after combining the tfidf body and title features (13686, 200)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "test_avgw2v_features = hstack((test_title_w2v, test_body_w2v))\n",
    "print('The shape of the combined data after combining the tfidf body and title features', test_avgw2v_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the Extracted features\n",
    "\n",
    "scipy.sparse.save_npz('D:/MajorProject_Code/Data/test_avgw2v.npz', test_avgw2v_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Pre-trained Word2Vec on Google News data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "googlenews_w2v = gensim.models.KeyedVectors.load_word2vec_format('D:/MajorProject_Code/Data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-89007675012c>:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  bodywords_google = list(googlenews_w2v.wv.vocab)\n"
     ]
    }
   ],
   "source": [
    "#list of words that google news pre-trained model is trained on\n",
    "\n",
    "bodywords_google = list(googlenews_w2v.wv.vocab)\n",
    "print(len(bodywords_google))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-8911982b9fc7>:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  googlenews_w2v.wv.most_similar('war')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('wars', 0.748465895652771),\n",
       " ('War', 0.6410670280456543),\n",
       " ('invasion', 0.5892110466957092),\n",
       " ('Persian_Gulf_War', 0.5890660285949707),\n",
       " ('Vietnam_War', 0.5886474847793579),\n",
       " ('Iraq', 0.588599443435669),\n",
       " ('unwinnable_quagmire', 0.5681803226470947),\n",
       " ('un_winnable', 0.560634970664978),\n",
       " ('occupation', 0.5506216287612915),\n",
       " ('conflict', 0.5506188273429871)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at how the model performs - testing it with some common words\n",
    "\n",
    "googlenews_w2v.wv.most_similar('war')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-70fdb3178fd6>:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  googlenews_w2v.wv.most_similar('Trump')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Donald_Trump', 0.8103920221328735),\n",
       " ('impersonator_entertained', 0.5942256450653076),\n",
       " ('Ivanka_Trump', 0.5924582481384277),\n",
       " ('Ivanka', 0.560720682144165),\n",
       " ('mogul_Donald_Trump', 0.5592452883720398),\n",
       " ('Trump_Tower', 0.5485552549362183),\n",
       " ('Kepcher', 0.5468589067459106),\n",
       " ('billionaire_Donald_Trump', 0.5447269678115845),\n",
       " ('Trumpster', 0.5412819981575012),\n",
       " ('tycoon_Donald_Trump', 0.5383971929550171)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at how the model performs - testing it with some common words\n",
    "\n",
    "googlenews_w2v.wv.most_similar('Trump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here we are trying to compare the pre-trained google news model to the one that was trained on our corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Creating an avg-word2vec on the title and body corpus from the pre-trained google-news data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54744\n",
      "300\n",
      "Wall time: 29min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# average Word2Vec for title\n",
    "\n",
    "title_googlew2v = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in lemmatized_title:# for each review/sentence\n",
    "    sent_vec = np.zeros(300) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in bodywords_google:\n",
    "            vec = googlenews_w2v.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    title_googlew2v.append(sent_vec)\n",
    "print(len(title_googlew2v))\n",
    "print(len(title_googlew2v[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13686\n",
      "300\n",
      "Wall time: 7min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# average Word2Vec for test title\n",
    "\n",
    "test_title_googlew2v = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in lemmatized_title_test:# for each review/sentence\n",
    "    sent_vec = np.zeros(300) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in bodywords_google:\n",
    "            vec = googlenews_w2v.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    test_title_googlew2v.append(sent_vec)\n",
    "print(len(test_title_googlew2v))\n",
    "print(len(test_title_googlew2v[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the avg-word2vec vectors locally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "title_w2vgoogle = scipy.sparse.csr_matrix(title_googlew2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<54744x300 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16418615 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_w2vgoogle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('D:/MajorProject_Code/Data/title_w2vgoogle.npz', title_w2vgoogle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting and saving the test vectors\n",
    "import scipy\n",
    "\n",
    "test_title_w2vgoogle = scipy.sparse.csr_matrix(test_title_googlew2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<13686x300 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4104262 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_title_w2vgoogle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('D:/MajorProject_Code/Data/test_title_w2vgoogle.npz', test_title_w2vgoogle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/54744 [00:00<?, ?it/s]<timed exec>:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 54744/54744 [15:23:37<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54744\n",
      "300\n",
      "Wall time: 15h 23min 37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# average Word2Vec for body\n",
    "\n",
    "body_googlew2v = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in tqdm(lemmatized_body):# for each review/sentence\n",
    "    sent_vec = np.zeros(300) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in bodywords_google:\n",
    "            vec = googlenews_w2v.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    body_googlew2v.append(sent_vec)\n",
    "print(len(body_googlew2v))\n",
    "print(len(body_googlew2v[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting and saving the train body vectors\n",
    "import scipy\n",
    "\n",
    "body_w2vgoogle = scipy.sparse.csr_matrix(body_googlew2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<54744x300 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16422551 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_w2vgoogle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('D:/MajorProject_Code/Data/body_w2vgoogle.npz', body_w2vgoogle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/13686 [00:00<?, ?it/s]<timed exec>:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 13686/13686 [3:52:27<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13686\n",
      "300\n",
      "Wall time: 3h 52min 27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# average Word2Vec for test body\n",
    "\n",
    "test_body_googlew2v = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent3 in tqdm(lemmatized_body_test):# for each review/sentence\n",
    "    sent_vec3 = np.zeros(300) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n",
    "    cnt_words3 =0; # num of words with a valid vector in the sentence/review\n",
    "    for word3 in sent3: # for each word in a review/sentence\n",
    "        if word3 in bodywords_google:\n",
    "            vec3 = googlenews_w2v.wv[word3]\n",
    "            sent_vec3 += vec3\n",
    "            cnt_words3 += 1\n",
    "    if cnt_words3 != 0:\n",
    "        sent_vec3 /= cnt_words3\n",
    "    test_body_googlew2v.append(sent_vec3)\n",
    "print(len(test_body_googlew2v))\n",
    "print(len(test_body_googlew2v[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting and saving the test body vectors\n",
    "import scipy\n",
    "\n",
    "test_body_w2vgoogle = scipy.sparse.csr_matrix(test_body_googlew2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<13686x300 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4105184 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_body_w2vgoogle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the test_body w2v google\n",
    "\n",
    "scipy.sparse.save_npz('D:/MajorProject_Code/Data/test_body_w2vgoogle.npz', test_body_w2vgoogle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_w2vgoogle = scipy.sparse.load_npz('D:/MajorProject_Code/Data/title_w2vgoogle.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title_w2vgoogle = scipy.sparse.load_npz('D:/MajorProject_Code/Data/test_title_w2vgoogle.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the combined data after combining the tfidf body and title features (54744, 600)\n"
     ]
    }
   ],
   "source": [
    "#combine the train extracted avg-w2v data\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "w2vgoogle = hstack((title_w2vgoogle, body_w2vgoogle))\n",
    "print('The shape of the combined data after combining the tfidf body and title features', w2vgoogle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the combined data after combining the tfidf body and title features (54744, 600)\n"
     ]
    }
   ],
   "source": [
    "#combine the test extracted avg-w2v data\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "test_w2vgoogle = hstack((test_title_w2vgoogle, test_body_w2vgoogle))\n",
    "print('The shape of the combined data after combining the tfidf body and title features', test_w2vgoogle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the combined data after combining the tfidf body and title features (13686, 600)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of the combined data after combining the tfidf body and title features', test_w2vgoogle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the combined train and test\n",
    "\n",
    "scipy.sparse.save_npz('D:/MajorProject_Code/Data/w2vgoogle.npz', w2vgoogle)\n",
    "scipy.sparse.save_npz('D:/MajorProject_Code/Data/test_w2vgoogle.npz', test_w2vgoogle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
